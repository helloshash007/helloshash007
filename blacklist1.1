import psycopg2
import subprocess
import json
import logging
from datetime import datetime
import time
import schedule
import os

# --- Configuration ---
DB_CONFIG = {
    "host": "your_postgres_host",
    "port": 5432,
    "database": "your_db",
    "user": "your_user",
    "password": "your_password"
}

TRANSLATOR_URL = "https://your-translator-url"
DATA_API_URL = "https://your-data-api-url"
USERNAME = "your_username"
PASSWORD = "your_password"
BATCH_SIZE = 200

# --- Logging Setup ---
log_filename = f"uid_sync_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(filename=log_filename, level=logging.INFO,
                    format='%(asctime)s [%(levelname)s] %(message)s')

# --- Log Helper ---
def log_json(title, obj):
    logging.info(f"{title}:\n{json.dumps(obj, indent=2)}")

# --- Get JWT Token from translator ---
def get_token():
    payload = {
        "input_token_state": {
            "token_type": "CREDENTIAL",
            "username": USERNAME,
            "password": PASSWORD
        },
        "output_token_state": {
            "token_type": "JWT"
        }
    }
    log_json("Translator API Payload", payload)

    cmd = [
        "curl", "-s", "-X", "POST", TRANSLATOR_URL,
        "-H", "Content-Type: application/json",
        "-d", json.dumps(payload)
    ]
    logging.info(f"Running curl command for token: {' '.join(cmd)}")

    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        logging.error(f"Translator call failed: {result.stderr}")
        raise Exception("Token request failed")

    response = json.loads(result.stdout)
    log_json("Translator API Response", response)

    return response["access_token"]

# --- Call Data API with user_refs ---
def get_user_data(token, user_refs):
    payload = {
        "filter criteria": [{
            "filter key": "GUID",
            "filter value": user_refs,
            "filterOpertaion": "in"
        }],
        "recordsPerPage": 200
    }

    headers = {
        "X-abc": "ABC",
        "X-E2E-Trust-Token": token,
        "Content-Type": "application/json"
    }

    log_json("Data API Payload", payload)
    logging.info(f"Headers: {headers}")

    cmd = [
        "curl", "-s", "-X", "POST", DATA_API_URL,
        "-H", f"X-abc: {headers['X-abc']}",
        "-H", f"X-E2E-Trust-Token: {token}",
        "-H", "Content-Type: application/json",
        "-d", json.dumps(payload)
    ]
    logging.info(f"Running curl command for data API: {' '.join(cmd)}")

    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        logging.error(f"Data API call failed: {result.stderr}")
        raise Exception("Data API call failed")

    try:
        response_json = json.loads(result.stdout)
        log_json("Data API Response", response_json)
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON response: {result.stdout[:500]}")
        return []

    return response_json.get("results", [])

# --- Main sync logic ---
def sync_users():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        conn.autocommit = False

        excluded_schemas = {"pg_catalog", "information_schema", "pg_toast", "pg_temp_1", "pg_toast_temp_1"}
        cursor.execute("SELECT schema_name FROM information_schema.schemata")
        schemas = [s[0] for s in cursor.fetchall() if not s[0].startswith("pg_") and s[0] not in excluded_schemas]

        user_ref_map = []
        for schema in schemas:
            try:
                cursor.execute(f"SELECT user_ref, first_name, last_name, email FROM {schema}.users")
                rows = cursor.fetchall()
                user_ref_map.extend([(schema, *row) for row in rows])
            except Exception as e:
                logging.warning(f"Skipping schema {schema} due to error: {e}")
                conn.rollback()

        token = get_token()

        for i in range(0, len(user_ref_map), BATCH_SIZE):
            batch = user_ref_map[i:i+BATCH_SIZE]
            refs = [b[1] for b in batch]
            data = get_user_data(token, refs)
            data_dict = {x["guid"]: x for x in data}

            for schema, user_ref, first, last, email in batch:
                user = data_dict.get(user_ref)
                if not user:
                    continue
                if user["first_name"] != first or user["last_name"] != last or user["email"] != email:
                    try:
                        cursor.execute(
                            f"UPDATE {schema}.users SET first_name=%s, last_name=%s, email=%s, updated_time=%s WHERE user_ref=%s",
                            (user["first_name"], user["last_name"], user["email"], datetime.utcnow(), user_ref)
                        )
                        logging.info(f"Updated user_ref {user_ref} in {schema}")
                    except Exception as e:
                        logging.error(f"Update failed for {user_ref} in {schema}: {e}")
                        conn.rollback()
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        logging.error(f"Sync failed: {e}. Retrying after 30 minutes.")
        time.sleep(1800)
        sync_users()

# --- Schedule the job every minute ---
schedule.every(1).minutes.do(sync_users)

if __name__ == "__main__":
    logging.info("User sync script started")
    while True:
        schedule.run_pending()
        time.sleep(1)